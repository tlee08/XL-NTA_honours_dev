{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/tim_details/tim_honours/CAPTURES/client_pi_50'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/timothylee/Desktop/Uni/Yr5/Honours/honours_dev/3_classify/CSI_CNN.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/timothylee/Desktop/Uni/Yr5/Honours/honours_dev/3_classify/CSI_CNN.ipynb#Y342sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ls \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39m\"\u001b[39;49m\u001b[39m/Volumes/tim_details/tim_honours/CAPTURES/client_pi_50\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timothylee/Desktop/Uni/Yr5/Honours/honours_dev/3_classify/CSI_CNN.ipynb#Y342sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ls\u001b[39m.\u001b[39msort()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/timothylee/Desktop/Uni/Yr5/Honours/honours_dev/3_classify/CSI_CNN.ipynb#Y342sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ls\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/tim_details/tim_honours/CAPTURES/client_pi_50'"
     ]
    }
   ],
   "source": [
    "ls = os.listdir(\"/Volumes/tim_details/tim_honours/CAPTURES/client_pi_50\")\n",
    "ls.sort()\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING IN DATA\n",
    "# NOTE: combining data in different steps to reduce the load of np.concatenate\n",
    "\n",
    "classes = [\n",
    "    \"client_pc_50\",\n",
    "    \"client_pc_200\",\n",
    "    \"client_pc_200_wall\",\n",
    "    \"client_pi_50\",\n",
    "    \"client_pi_200\",\n",
    "    \"client_pi_200_wall\"\n",
    "]\n",
    "inner_dirs = [\n",
    "    'v=3InbMow9IYo',\n",
    "    \"v=A3gUpodXMv0\",\n",
    "    \"v=NSW5u1RTxEA\",\n",
    "    \"v=gxxqdrrpgZc\",\n",
    "    \"v=mkWKZWMokdI\",\n",
    "    \"v=t634q_Voeto\",\n",
    "    \"v=t6jlhqNxRYk\",\n",
    "    \"v=w_oGIbFjiCo\",\n",
    "    \"v=yve6qo6eowU\"\n",
    "]\n",
    "dir = \"/Volumes/tim_details/tim_honours/CAPTURES/\"\n",
    "\n",
    "# Init total X and y arrays\n",
    "X = np.zeros((0,500,64))\n",
    "y = np.zeros((0))\n",
    "# Iterating through all the dev, loc classes\n",
    "for c in classes:\n",
    "    # Getting dev, loc dir\n",
    "    dir_c = os.path.join(dir, c)\n",
    "    # Init dev, loc X and y arrays\n",
    "    X_c = np.zeros((0,500,64))\n",
    "    y_c = np.zeros((0))\n",
    "    # Iterating through all the vid classes\n",
    "    for i in inner_dirs:\n",
    "        print(c, i)\n",
    "        # Getting dev, loc, vid dir and csi fp\n",
    "        dir_c_i = os.path.join(dir_c, i)\n",
    "        csi_fp = os.path.join(dir_c_i, \"csi_all.npy\")\n",
    "        # Loading in csi npy file\n",
    "        X_c_i = np.load(csi_fp)\n",
    "        y_c_i = np.repeat(c, X_c_i.shape[0])\n",
    "        # Trimming data (reduced computation)\n",
    "        X_c_i = X_c_i[:, 2000:2500, :]\n",
    "        # Combining vid data into dev, loc X and y arrays\n",
    "        x_c = np.concatenate([X_c, X_c_i], axis=0)\n",
    "        y_c = np.concatenate([y_c, y_c_i], axis=0)\n",
    "    # Combining dev, loc data into total X and y arrays\n",
    "    X = np.concatenate([X, X_c], axis=0)\n",
    "    y = np.concatenate([y, y_c], axis=0)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING DATA\n",
    "\n",
    "# REMOVING CAPTURES WITH LESS THAN X FRAMES\n",
    "# Getting whether any frame (all subc in the frame) are non-zero (thus valid frame)\n",
    "# then finding the number of valid frames for each capture.\n",
    "# If the number of valid frames is below thresh, then it is an invalid capture (deleted)\n",
    "thresh = 400\n",
    "samples_to_remove = np.any(X != 0, axis=2).sum(axis=1) < thresh\n",
    "# Filtering out invalid frames\n",
    "X_cleaned = np.delete(X, samples_to_remove, axis=0)\n",
    "y_cleaned = np.delete(y, samples_to_remove, axis=0)\n",
    "\n",
    "# Filtering out null and pilot frames\n",
    "nulls = [0, 1, 2, 3, 4, 5, 63, 62, 61, 60, 59, 32]\n",
    "pilots = [11, 25, 53, 39]\n",
    "null_pilots = np.concatenate([nulls, pilots])\n",
    "X_cleaned = np.delete(X_cleaned, null_pilots, axis=2)\n",
    "\n",
    "print(X_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "\n",
    "# Encode the labels\n",
    "# le = LabelEncoder()\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_cleaned)\n",
    "y_lb = lb.transform(y_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "\n",
    "# Log transforming and MinMax scaling the data\n",
    "X_procd = X_cleaned\n",
    "# X_procd = np.sqrt(X_procd)\n",
    "# x_procd = np.log1p(x_procd)\n",
    "# x_procd = (x_procd)/np.abs(x_procd).max()\n",
    "# x_procd = MinMaxScaler().fit_transform(x_procd.reshape(-1, 1)).reshape(*x_procd.shape)\n",
    "\n",
    "# Reshaping arrays to expected dimensions (samples * x_dim * y_dim * channels)\n",
    "# x_procd = x_procd.reshape(*x_procd.shape, 1)\n",
    "\n",
    "# Converting the complex numbers to an array with 2 channels\n",
    "X_procd_temp = np.zeros((*X_procd.shape, 1))\n",
    "X_procd_temp[:,:,:,0] = np.abs(X_procd) / np.abs(X_procd).max()\n",
    "# x_procd_temp[:,:,:,1] = np.angle(x_procd) / (2*np.pi) + 0.5\n",
    "X_procd = X_procd_temp\n",
    "\n",
    "X_procd = np.log1p(X_procd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "\n",
    "# Showing distribution of all csi amplitude values\n",
    "plt.hist(\n",
    "    # x_procd[y_cleaned==\"client_pc_200\"].flatten(),\n",
    "    X_procd.flatten(),\n",
    "    bins=40,\n",
    "    )\n",
    "plt.show()\n",
    "\n",
    "# SENSE CHECKING EXAMPLE CSI HEATMAP IMAGE\n",
    "im = plt.pcolormesh(\n",
    "    np.abs(X_procd[198, :, :, 0]),\n",
    ")\n",
    "plt.colorbar(im)\n",
    "plt.show()\n",
    "\n",
    "my_col = colormaps['Spectral']\n",
    "for i in np.arange(180, 190):\n",
    "    plt.plot(\n",
    "        np.abs(X_procd[i, :, :, 0]).T,\n",
    "        color=my_col((i-180)/10),\n",
    "        alpha=1\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_procd,\n",
    "    y_lb,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_lb,\n",
    ")\n",
    "\n",
    "# Flattening this data (reshaping) to feed into different ML algos\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PERFORMING PCA ANALYSIS (IN-CASE)\n",
    "# Initialize and fit the PCA model\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(np.abs(X_train_flat))\n",
    "\n",
    "# Transform the data to the lower-dimensional space (and selecting only the top 3 components)\n",
    "n_components = 5\n",
    "X_train_pca = pca.transform(np.abs(X_train_flat))[:, :n_components]\n",
    "X_test_pca = pca.transform(np.abs(X_test_flat))[:, :n_components]\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.vlines(x=n_components, ymin=0, ymax=pca.explained_variance_ratio_.max(), colors=(0.6,0,0.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[155   4   1   0   1   0]\n",
      " [  1 143   0   0   2   0]\n",
      " [  9   1 166   0   0   0]\n",
      " [  0   0   0 150   8   0]\n",
      " [  0   3   0   9 159   2]\n",
      " [  0   0   0   6  10 176]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "      client_pc_50       0.96      0.93      0.95       165\n",
      "     client_pc_200       0.98      0.95      0.96       151\n",
      "client_pc_200_wall       0.94      0.99      0.97       167\n",
      "      client_pi_50       0.95      0.91      0.93       165\n",
      "     client_pi_200       0.92      0.88      0.90       180\n",
      "client_pi_200_wall       0.92      0.99      0.95       178\n",
      "\n",
      "         micro avg       0.94      0.94      0.94      1006\n",
      "         macro avg       0.95      0.94      0.94      1006\n",
      "      weighted avg       0.94      0.94      0.94      1006\n",
      "       samples avg       0.94      0.94      0.94      1006\n",
      "\n",
      "0.9423459244532804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timothylee/miniconda3/envs/honours_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# KNN WORKS VERY WELL!! Fast with PCA.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Making KNN\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=3,\n",
    "    weights='uniform',\n",
    "    algorithm='auto',\n",
    "    leaf_size=30,\n",
    "    p=2,\n",
    "    metric='minkowski',\n",
    "    metric_params=None,\n",
    "    n_jobs=None\n",
    ")\n",
    "\n",
    "# Training KNN\n",
    "# knn.fit(X_train_flat, y_train)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "\n",
    "# Evaluating KNN\n",
    "# y_pred = knn.predict(X_test_flat)\n",
    "y_pred = knn.predict(X_test_pca)\n",
    "\n",
    "# Showing evaluation confusion matrix\n",
    "cm = confusion_matrix(\n",
    "    lb.inverse_transform(y_pred),\n",
    "    lb.inverse_transform(y_test),\n",
    ") # cols = actual, rows = predicted\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier with RBF. Not as fast or accurate as KNN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Making RBF SVM\n",
    "svc = SVC(\n",
    "    C=2.0, # Regularisation parameter. Reg strength is inversely proportional to C\n",
    "    kernel='rbf', # {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable\n",
    "    # degree=3, # Degree for poly kernels\n",
    "    gamma='scale', # {‘scale’, ‘auto’} or float\n",
    "    coef0=0.0, # Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.\n",
    "    shrinking=True, # Whether to use the shrinking heuristic\n",
    "    probability=False, # Allows predict_proba but slows down process\n",
    "    tol=0.001, # Tolerance for stopping criterion.\n",
    "    cache_size=200, # Specify the size of the kernel cache in MB\n",
    "    class_weight=None, # Set the parameter C of class i to class_weight[i]*C. Keep as none for equal weights across classes\n",
    "    verbose=False, # Enable verbose output\n",
    "    max_iter=-1, # Hard limit on iterations within solver, or -1 for no limit\n",
    "    decision_function_shape='ovo', # {‘ovo’, ‘ovr’}\n",
    "    break_ties=False,\n",
    "    random_state=None\n",
    ")\n",
    "svc_mc = OneVsRestClassifier(svc)\n",
    "\n",
    "# Training KNN\n",
    "# svc_mc.fit(X_train_flat, y_train)\n",
    "svc_mc.fit(X_train_pca, y_train)\n",
    "\n",
    "# Evaluating KNN\n",
    "# y_pred = svc_mc.predict(X_test_flat)\n",
    "y_pred = svc_mc.predict(X_test_pca)\n",
    "\n",
    "# Showing evaluation confusion matrix\n",
    "cm = confusion_matrix(\n",
    "    lb.inverse_transform(y_pred),\n",
    "    lb.inverse_transform(y_test),\n",
    ")\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Making MLP model with Entire CSI matrix.\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=X_train.shape[1:]),\n",
    "    layers.Conv2D(16, (10, 24), (5, 2), padding=\"valid\", activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(16, (3, 3), 2, padding=\"valid\", activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(lb.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Show model architecture\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = model.predict(X_test)\n",
    "# y_pred = tf.keras.utils.to_categorical(y_pred.argmax(axis=1), len(lb.classes_)).astype(int)\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    lb.inverse_transform(y_pred),\n",
    "    lb.inverse_transform(y_test)\n",
    ")\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.8400 - categorical_accuracy: 0.6403 - val_loss: 0.5266 - val_categorical_accuracy: 0.7913\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 0s 1ms/step - loss: 0.5466 - categorical_accuracy: 0.7833 - val_loss: 0.3624 - val_categorical_accuracy: 0.8522\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 0s 1ms/step - loss: 0.4603 - categorical_accuracy: 0.8287 - val_loss: 0.3455 - val_categorical_accuracy: 0.8634\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 0s 1ms/step - loss: 0.4173 - categorical_accuracy: 0.8505 - val_loss: 0.3281 - val_categorical_accuracy: 0.8857\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 0s 1ms/step - loss: 0.3721 - categorical_accuracy: 0.8701 - val_loss: 0.2848 - val_categorical_accuracy: 0.9068\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 0s 1ms/step - loss: 0.3862 - categorical_accuracy: 0.8583 - val_loss: 0.3299 - val_categorical_accuracy: 0.8932\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 0s 1ms/step - loss: 0.3650 - categorical_accuracy: 0.8747 - val_loss: 0.3227 - val_categorical_accuracy: 0.8981\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 0s 1ms/step - loss: 0.3633 - categorical_accuracy: 0.8726 - val_loss: 0.2744 - val_categorical_accuracy: 0.9106\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 0s 1ms/step - loss: 0.3533 - categorical_accuracy: 0.8772 - val_loss: 0.2908 - val_categorical_accuracy: 0.9056\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 0s 2ms/step - loss: 0.3275 - categorical_accuracy: 0.8816 - val_loss: 0.2636 - val_categorical_accuracy: 0.9056\n",
      "126/126 [==============================] - 0s 745us/step\n",
      "[[610  58  11   0   0   0]\n",
      " [  7 541   0   1  15   0]\n",
      " [ 45   2 657   0   0   0]\n",
      " [  0   0   0 628  53   0]\n",
      " [  0   1   0  20 553   6]\n",
      " [  0   0   0  13  96 705]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Making MLP model with PCA\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=X_train_pca.shape[1:]),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(len(lb.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    # loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        # tf.keras.metrics.Accuracy(),\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Show model architecture\n",
    "# model.summary()\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    X_train_pca,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = model.predict(X_train_pca)\n",
    "y_pred = tf.keras.utils.to_categorical(y_pred.argmax(axis=1), len(lb.classes_)).astype(int)\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    lb.inverse_transform(y_pred),\n",
    "    lb.inverse_transform(y_train)\n",
    ")\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step\n",
      "[[148  15   2   0   0   0]\n",
      " [  3 134   0   0   0   0]\n",
      " [ 14   2 165   0   0   0]\n",
      " [  0   0   0 152  17   0]\n",
      " [  0   0   0   4 138   4]\n",
      " [  0   0   0   9  25 174]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "      client_pc_50       0.90      0.90      0.90       165\n",
      "     client_pc_200       0.98      0.89      0.93       151\n",
      "client_pc_200_wall       0.91      0.99      0.95       167\n",
      "      client_pi_50       0.90      0.92      0.91       165\n",
      "     client_pi_200       0.95      0.77      0.85       180\n",
      "client_pi_200_wall       0.84      0.98      0.90       178\n",
      "\n",
      "         micro avg       0.91      0.91      0.91      1006\n",
      "         macro avg       0.91      0.91      0.91      1006\n",
      "      weighted avg       0.91      0.91      0.90      1006\n",
      "       samples avg       0.91      0.91      0.91      1006\n",
      "\n",
      "0.9055666003976143\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "y_pred = model.predict(X_test_pca)\n",
    "y_pred = tf.keras.utils.to_categorical(y_pred.argmax(axis=1), len(lb.classes_)).astype(int)\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    lb.inverse_transform(y_pred),\n",
    "    lb.inverse_transform(y_test)\n",
    ")\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honours_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
