{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory storing the CSI and WLAN captures\n",
    "resources_dir = \"/Volumes/tim_details/tim_honours/CAPTURES\"\n",
    "\n",
    "# Directory to save plots to\n",
    "plt_dir = \"/Users/timothylee/Desktop/Uni/Yr5/Honours/honours_thesis/figures/plt_figs/\"\n",
    "\n",
    "# Supress pd scientific notation\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "\n",
    "# Resolution of plots\n",
    "plt.rcParams[\"figure.dpi\"] = 100 # 300\n",
    "# plt.rcParams[\"figure.dpi\"] = 500 # 300\n",
    "\n",
    "# Backend to generate plots\n",
    "# mpl.use(\"agg\")\n",
    "# %matplotlib ipympl\n",
    "%matplotlib inline\n",
    "\n",
    "# plt figure style\n",
    "fig_style = \"seaborn-v0_8-whitegrid\"\n",
    "\n",
    "# colormaps\n",
    "cmap_qual = \"pastel\"\n",
    "cmap_seq = \"viridis\"\n",
    "cmap_cycl = \"twilight\"\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in total binned df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_hdf(\n",
    "    os.path.join(resources_dir, \"total_wlan.h5\"),\n",
    "    key=\"wlan\",\n",
    "    mode=\"r\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting total binned df as features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making each row instance's time series vector for each column's measure\n",
    "X_features = (\n",
    "    X\n",
    "    .sort_values(\"ts_bins\")\n",
    "    .groupby([\"devices\", \"videos\", \"instances\"])\n",
    "    .agg({\n",
    "        # Uplink (with non-data, data, and all frames)\n",
    "        \"frames_up_ndat\": lambda x: x.values.tolist(),\n",
    "        \"bytes_up_ndat\": lambda x: x.values.tolist(),\n",
    "        \"frames_up_dat\": lambda x: x.values.tolist(),\n",
    "        \"bytes_up_dat\": lambda x: x.values.tolist(),\n",
    "        \"frames_up_all\": lambda x: x.values.tolist(),\n",
    "        \"bytes_up_all\": lambda x: x.values.tolist(),\n",
    "        # Downlink\n",
    "        \"frames_dn_ndat\": lambda x: x.values.tolist(),\n",
    "        \"bytes_dn_ndat\": lambda x: x.values.tolist(),\n",
    "        \"frames_dn_dat\": lambda x: x.values.tolist(),\n",
    "        \"bytes_dn_dat\": lambda x: x.values.tolist(),\n",
    "        \"frames_dn_all\": lambda x: x.values.tolist(),\n",
    "        \"bytes_dn_all\": lambda x: x.values.tolist(),\n",
    "        # All\n",
    "        \"frames_all_ndat\": lambda x: x.values.tolist(),\n",
    "        \"bytes_all_ndat\": lambda x: x.values.tolist(),\n",
    "        \"frames_all_dat\": lambda x: x.values.tolist(),\n",
    "        \"bytes_all_dat\": lambda x: x.values.tolist(),\n",
    "        \"frames_all_all\": lambda x: x.values.tolist(),\n",
    "        \"bytes_all_all\": lambda x: x.values.tolist(),\n",
    "    })\n",
    ")\n",
    "\n",
    "X_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "\n",
    "# Making a DF of the corresponding label combinations of each sample\n",
    "Y = (\n",
    "    X_features\n",
    "    .index\n",
    "    .to_frame()\n",
    "    .reset_index(drop=True)\n",
    "    .assign(\n",
    "        locations=lambda x: x[\"devices\"].str.split(\" \").str[1:].str.join(\" \"),\n",
    "        devices=lambda x: x[\"devices\"].str.split(\" \").str[0],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Using video as label\n",
    "labels_to_classify = [\"videos\", \"devices\"]\n",
    "# TODO: can try out different combinations\n",
    "y = np.array(\n",
    "    [\"|\".join(x) for x in Y[labels_to_classify].values]\n",
    ")\n",
    "\n",
    "# Encode the labels\n",
    "# le = LabelEncoder() # assigns integer encoding\n",
    "lb = LabelBinarizer() # one-hot encoding|\n",
    "lb.fit(y)\n",
    "y_lb = lb.transform(y)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing X features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "\n",
    "# Making a 3D features matrix\n",
    "feature_channels = ['frames_all_ndat', 'frames_all_dat', 'bytes_all_all']\n",
    "# Axes format is (instance, time, feature_channel)\n",
    "X_features_matr = (\n",
    "    np.array(\n",
    "        X_features[feature_channels].values.tolist()\n",
    "    )\n",
    "    .transpose(0, 2, 1)\n",
    ")\n",
    "\n",
    "# MinMax scaling the 2D matrix of each feature channel\n",
    "X_features_matr_scaled = np.zeros(X_features_matr.shape)\n",
    "# For each feature channel\n",
    "for i in np.arange(X_features_matr.shape[2]):\n",
    "    # Scale the 2D (instance, time) matrix\n",
    "    view = X_features_matr[:, :, i]\n",
    "    X_features_matr_scaled[:, :, i] = (view - view.min())/(view.max() - view.min())\n",
    "# Set nan values to 0\n",
    "X_features_matr_scaled[np.isnan(X_features_matr_scaled)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising features to sense check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plots through time for some labels\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "axes = fig.subplots(nrows=4, ncols=X_features_matr_scaled.shape[2]).reshape(-1, X_features_matr_scaled.shape[2])\n",
    "# For each label (given n labels)\n",
    "for i in np.arange(axes.shape[0]):\n",
    "    # For each feature channel\n",
    "    for j in np.arange(axes.shape[1]):\n",
    "        lab = np.unique(y)[i]\n",
    "        feature = feature_channels[j]\n",
    "        axes[i, j].plot(\n",
    "            X_features_matr_scaled[y == lab][:5, :, j].T\n",
    "        )\n",
    "        axes[i, j].set_title(f\"{lab}, {feature}\")\n",
    "\n",
    "# Hist plots of each feature's value frequency for some instances\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "axes = fig.subplots(nrows=4, ncols=X_features_matr_scaled.shape[2]).reshape(-1, X_features_matr_scaled.shape[2])\n",
    "# For each label (given n labels)\n",
    "for i in np.arange(axes.shape[0]):\n",
    "    # For each feature channel\n",
    "    for j in np.arange(axes.shape[1]):\n",
    "        lab = np.unique(y)[i]\n",
    "        feature = feature_channels[j]\n",
    "        sns.histplot(\n",
    "            (X_features_matr[0, :, j]),\n",
    "            element=\"step\",\n",
    "            bins=50,\n",
    "            ax=axes[i, j],\n",
    "        )\n",
    "        axes[i, j].set_ylim(0, 10)\n",
    "        axes[i, j].set_title(f\"{lab}, {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features_matr_scaled,\n",
    "    y_lb,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_lb,\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Flattening this data (reshaping) to feed into different ML algos\n",
    "# X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "# X_test_flat = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conducting PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PERFORMING PCA ANALYSIS (IN-CASE)\n",
    "# Initialize and fit the PCA model\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(np.abs(X_train))\n",
    "\n",
    "# Transform the data to the lower-dimensional space (and selecting only the top 3 components)\n",
    "n_components = 2\n",
    "X_train_pca = pca.transform(np.abs(X_train))[:, :n_components]\n",
    "X_test_pca = pca.transform(np.abs(X_test))[:, :n_components]\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.vlines(x=n_components, ymin=0, ymax=pca.explained_variance_ratio_.max(), colors=(0.6,0,0.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    cm,\n",
    "    classes,\n",
    "    title='Confusion matrix',\n",
    "    cmap=cmap_intsy,\n",
    "):\n",
    "    \"\"\"\n",
    "    To plot heatmap of confusion matrix\n",
    "    \"\"\"\n",
    "    # Initialising figure and axes\n",
    "    with plt.style.context(fig_style):\n",
    "        fig = plt.figure(\n",
    "            figsize=(8, 8),\n",
    "            layout=\"constrained\"\n",
    "        )\n",
    "        ax = fig.subplots()\n",
    "    # Making confusion matrix heatmap\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        # annot=True,\n",
    "        ax=ax,\n",
    "        cmap=cmap,\n",
    "        fmt='.2f',\n",
    "        # cbar=False,\n",
    "        xticklabels=lb.classes_,\n",
    "        yticklabels=lb.classes_,\n",
    "    )\n",
    "    # Set titles\n",
    "    ax.tick_params(labelsize=\"small\")\n",
    "    ax.set_title(title, fontsize=\"xx-large\")\n",
    "    ax.set_xlabel(\"Predicted\", fontsize=\"large\")\n",
    "    ax.set_ylabel(\"True\", fontsize=\"large\")\n",
    "    # Return figure and axis\n",
    "    return fig, ax\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_model(\n",
    "    model,\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    name,\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves the model and results to a folder (given by name)\n",
    "    \"\"\"\n",
    "    # Making directory to store model\n",
    "    my_model_dir = os.path.join(models_dir, name)\n",
    "    os.makedirs(my_model_dir, exist_ok=True)\n",
    "    # Storing model as pickle\n",
    "    model_fp = os.path.join(my_model_dir, f\"{name}.pkl\")\n",
    "    with open(model_fp, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    # Generating and storing results as pickle\n",
    "    res_fp = os.path.join(my_model_dir, f\"{name}.h5\")\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"y_true\": lb.inverse_transform(y_test),\n",
    "            \"y_pred\": lb.inverse_transform(y_pred),\n",
    "        }\n",
    "    ).to_hdf(res_fp, key=\"results\", mode=\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier with RBF. Not as fast or accurate as KNN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Making RBF SVM\n",
    "svc = SVC(\n",
    "    C=2.0, # Regularisation parameter. Reg strength is inversely proportional to C\n",
    "    kernel='rbf', # {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable\n",
    "    # degree=3, # Degree for poly kernels\n",
    "    gamma='scale', # {‘scale’, ‘auto’} or float\n",
    "    coef0=0.0, # Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.\n",
    "    shrinking=True, # Whether to use the shrinking heuristic\n",
    "    probability=False, # Allows predict_proba but slows down process\n",
    "    tol=0.001, # Tolerance for stopping criterion.\n",
    "    cache_size=200, # Specify the size of the kernel cache in MB\n",
    "    class_weight=None, # Set the parameter C of class i to class_weight[i]*C. Keep as none for equal weights across classes\n",
    "    verbose=False, # Enable verbose output\n",
    "    max_iter=-1, # Hard limit on iterations within solver, or -1 for no limit\n",
    "    decision_function_shape='ovo', # {‘ovo’, ‘ovr’}\n",
    "    break_ties=False,\n",
    "    random_state=None\n",
    ")\n",
    "svc_mc = OneVsRestClassifier(svc)\n",
    "\n",
    "# Training KNN\n",
    "svc_mc.fit(X_train, y_train)\n",
    "# svc_mc.fit(X_train_pca, y_train)\n",
    "\n",
    "# Evaluating KNN\n",
    "y_pred = svc_mc.predict(X_test)\n",
    "# y_pred = svc_mc.predict(X_test_pca)\n",
    "\n",
    "# Showing evaluation confusion matrix\n",
    "cm = confusion_matrix(\n",
    "    lb.inverse_transform(y_pred),\n",
    "    lb.inverse_transform(y_test),\n",
    ")\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred, target_names=lb.classes_))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Making MLP model with Entire CSI matrix.\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1], 1)),\n",
    "    layers.Conv1D(8, 5, padding=\"valid\", activation='relu'),\n",
    "    layers.MaxPooling1D(2),\n",
    "    layers.Conv1D(16, (3,), padding=\"valid\", activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(lb.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Show model architecture\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = tf.keras.utils.to_categorical(y_pred.argmax(axis=1), len(lb.classes_)).astype(int)\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    lb.inverse_transform(y_pred),\n",
    "    lb.inverse_transform(y_test)\n",
    ")\n",
    "print(cm)\n",
    "# print(classification_report(y_test, y_pred, target_names=classes))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Making MLP model with PCA\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1], 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(len(lb.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    # loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        # tf.keras.metrics.Accuracy(),\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Show model architecture\n",
    "# model.summary()\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = model.predict(X_train)\n",
    "y_pred = tf.keras.utils.to_categorical(y_pred.argmax(axis=1), len(lb.classes_)).astype(int)\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    lb.inverse_transform(y_pred),\n",
    "    lb.inverse_transform(y_train)\n",
    ")\n",
    "print(cm)\n",
    "print(classification_report(y_train, y_pred, target_names=lb.classes_))\n",
    "print(accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "y_pred = model.predict(X_test_pca)\n",
    "y_pred = tf.keras.utils.to_categorical(y_pred.argmax(axis=1), len(lb.classes_)).astype(int)\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    lb.inverse_transform(y_pred),\n",
    "    lb.inverse_transform(y_test)\n",
    ")\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honours_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
